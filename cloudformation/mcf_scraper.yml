---
AWSTemplateFormatVersion: '2010-09-09'
Description: 'MCF Scraper Step Functions'

Resources:
  # Step functions state machine
  MCFScraperSM:
    Type: 'AWS::StepFunctions::StateMachine'
    Properties:
      DefinitionString: !Sub |
        {
          "Comment": "A description of my state machine",
          "StartAt": "Extract_Max_Search",
          "States": {
            "Extract_Max_Search": {
              "Type": "Task",
              "Resource": "${GetMaxSearch.Arn}",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException"
                  ],
                  "IntervalSeconds": 2,
                  "MaxAttempts": 6,
                  "BackoffRate": 2
                }
              ],
              "Next": "Get_Raw_Data_Parallelize"
            },
            "Get_Raw_Data_Parallelize": {
              "Type": "Parallel",
              "Branches": [
                {
                  "StartAt": "Get_Raw_Data_1",
                  "States": {
                    "Get_Raw_Data_1": {
                      "Type": "Task",
                      "Resource": "${GetRawData.Arn}",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException"
                          ],
                          "IntervalSeconds": 2,
                          "MaxAttempts": 6,
                          "BackoffRate": 2
                        }
                      ],
                      "End": true,
                      "InputPath": "$.Load_1"
                    }
                  }
                },
                {
                  "StartAt": "Get_Raw_Data_2",
                  "States": {
                    "Get_Raw_Data_2": {
                      "Type": "Task",
                      "Resource": "${GetRawData.Arn}",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException"
                          ],
                          "IntervalSeconds": 2,
                          "MaxAttempts": 6,
                          "BackoffRate": 2
                        }
                      ],
                      "End": true,
                      "InputPath": "$.Load_2"
                    }
                  }
                },
                {
                  "StartAt": "Get_Raw_Data_3",
                  "States": {
                    "Get_Raw_Data_3": {
                      "Type": "Task",
                      "Resource": "${GetRawData.Arn}",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException"
                          ],
                          "IntervalSeconds": 2,
                          "MaxAttempts": 6,
                          "BackoffRate": 2
                        }
                      ],
                      "End": true,
                      "InputPath": "$.Load_3"
                    }
                  }
                },
                {
                  "StartAt": "Get_Raw_Data_4",
                  "States": {
                    "Get_Raw_Data_4": {
                      "Type": "Task",
                      "Resource": "${GetRawData.Arn}",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException"
                          ],
                          "IntervalSeconds": 2,
                          "MaxAttempts": 6,
                          "BackoffRate": 2
                        }
                      ],
                      "End": true,
                      "InputPath": "$.Load_4"
                    }
                  }
                },
                {
                  "StartAt": "Get_Raw_Data_5",
                  "States": {
                    "Get_Raw_Data_5": {
                      "Type": "Task",
                      "Resource": "${GetRawData.Arn}",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException"
                          ],
                          "IntervalSeconds": 30,
                          "MaxAttempts": 2,
                          "BackoffRate": 2
                        }
                      ],
                      "End": true,
                      "InputPath": "$.Load_5"
                    }
                  }
                },
                {
                  "StartAt": "Get_Raw_Data_6",
                  "States": {
                    "Get_Raw_Data_6": {
                      "Type": "Task",
                      "Resource": "${GetRawData.Arn}",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException"
                          ],
                          "IntervalSeconds": 30,
                          "MaxAttempts": 2,
                          "BackoffRate": 2
                        }
                      ],
                      "End": true,
                      "InputPath": "$.Load_6"
                    }
                  }
                },
                {
                  "StartAt": "Get_Raw_Data_7",
                  "States": {
                    "Get_Raw_Data_7": {
                      "Type": "Task",
                      "Resource": "${GetRawData.Arn}",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException"
                          ],
                          "IntervalSeconds": 30,
                          "MaxAttempts": 2,
                          "BackoffRate": 2
                        }
                      ],
                      "End": true,
                      "InputPath": "$.Load_7"
                    }
                  }
                },
                {
                  "StartAt": "Get_Raw_Data_8",
                  "States": {
                    "Get_Raw_Data_8": {
                      "Type": "Task",
                      "Resource": "${GetRawData.Arn}",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException"
                          ],
                          "IntervalSeconds": 30,
                          "MaxAttempts": 2,
                          "BackoffRate": 2
                        }
                      ],
                      "End": true,
                      "InputPath": "$.Load_8"
                    }
                  }
                }
              ],
              "Next": "Transform_New_Job_N_Meta"
            },
            "Transform_New_Job_N_Meta": {
              "Type": "Task",
              "Resource": "${TransformRawData.Arn}",
              "OutputPath": "$.Payload",
              "Parameters": {
                "Payload.$": "$"
              },
              "Retry": [
                {
                  "ErrorEquals": [
                    "Lambda.ServiceException",
                    "Lambda.AWSLambdaException",
                    "Lambda.SdkClientException"
                  ],
                  "IntervalSeconds": 30,
                  "MaxAttempts": 2,
                  "BackoffRate": 2
                }
              ],
              "End": true
            }
          }
        }
      RoleArn: !GetAtt 'MCFScraperStateMachine.Arn'


  # Lambda functions
  GetMaxSearch: # no monitoring needed because function is invoked by StepFunctions
    Type: 'AWS::Lambda::Function'
    Properties:
      MemorySize: 128
      Code:
        ZipFile: |
          import requests
          import numpy as np

          def lambda_handler(event, context):
              """
              This function is use to generate the list of number of new job page, and the return list would be use in threadpoolexecutor
              """
              url = 'https://api.mycareersfuture.gov.sg/v2/jobs?limit=20&page=0&sortBy=new_posting_date'
              result_temp = requests.get(url).json()
              web_total_page = result_temp['total']//20

              web_total_page_list = []
              for each in range(web_total_page):
                  web_total_page_list.append(each)
              
              partitions = 8
              splitted_list = np.array_split(web_total_page_list,partitions)
              
              return {
                'Payload': {
                  'Load_1': {
                    's3_key':'split_1',
                    'start':f'{splitted_list[0][0]}',
                    'end':f'{splitted_list[0][-1]}'
                  },
                  'Load_2': {
                    's3_key':'split_2',
                    'start':f'{splitted_list[1][0]}',
                    'end':f'{splitted_list[1][-1]}'
                  },
                  'Load_3': {
                    's3_key':'split_3',
                    'start':f'{splitted_list[2][0]}',
                    'end':f'{splitted_list[2][-1]}'
                  },
                  'Load_4': {
                    's3_key':'split_4',
                    'start':f'{splitted_list[3][0]}',
                    'end':f'{splitted_list[3][-1]}'
                  },
                  'Load_5': {
                    's3_key':'split_5',
                    'start':f'{splitted_list[4][0]}',
                    'end':f'{splitted_list[4][-1]}'
                  },
                  'Load_6': {
                    's3_key':'split_6',
                    'start':f'{splitted_list[5][0]}',
                    'end':f'{splitted_list[5][-1]}'
                  },
                  'Load_7': {
                    's3_key':'split_7',
                    'start':f'{splitted_list[6][0]}',
                    'end':f'{splitted_list[6][-1]}'
                  },
                  'Load_8': {
                    's3_key':'split_8',
                    'start':f'{splitted_list[7][0]}',
                    'end':f'{splitted_list[7][-1]}'
                  }
                }
              }
      Handler: 'index.lambda_handler'
      Layers: 
        - !Sub 'arn:aws:lambda:${AWS::Region}:336392948345:layer:AWSDataWrangler-Python39:3'
      Timeout: 900
      Role: !GetAtt 'ETLLambdaIAM.Arn'
      Runtime: 'python3.9'
  GetRawData: # no monitoring needed because function is invoked by StepFunctions
    Type: 'AWS::Lambda::Function'
    Properties:
      MemorySize: 1280
      Code:
        ZipFile: |
          import pandas as pd
          import requests
          import datetime
          import numpy as np
          import awswrangler as wr

          def get_handles_new_link(splitted_list: list, s3_key: str):
              """
              The function that would run in multi-threaded function 'setup_threaded_workers_search'
              """
              output_scrapped_df = pd.DataFrame()
              for each_page in splitted_list:
                  print(each_page)
                  try:
                      url = ('https://api.mycareersfuture.gov.sg/v2/jobs?limit=20&page=' + str(each_page) + '&sortBy=new_posting_date')
                      json_temp = requests.get(url).json()
                      temp_scrapped_df = collect_data_json(json_temp)
                      frames = [output_scrapped_df, temp_scrapped_df]
                      output_scrapped_df = pd.concat(frames)
                  except:
                      print('error in downloading or concat data')
              s3_save_instance_df(output_scrapped_df, s3_key)

          def s3_save_instance_df(input_df: pd.DataFrame, s3_key: str):
              """
              aws s3 saving of scrapped data base on individual run time
              this approach would create single csv file every day or append to existing file
              """
              # setting the filler for the naming convention
              now = datetime.datetime.now() + datetime.timedelta(hours=8)
              # setting the S3 bucket name
              aws_s3_bucket = 'mcf'
              
              # setting the S3 object name / file name
              key = f'temp/{s3_key}'
              
              wr.s3.to_parquet(
                  df=input_df,
                  path=f"s3://{aws_s3_bucket}/{key}",
                  dataset=True,
                  mode="overwrite")
              
              print(f'File {key} saved')

          def collect_data_json(input_json: str) -> pd.DataFrame:
              """
              This function would scrape the website from the json data and output it in a DataFrame format.
              """
              temp_json = input_json['results']
              output_df = pd.json_normalize(temp_json)

              return output_df

          def lambda_handler(event, context):
              print(event)

              s3_key = event['s3_key']
              start = event['start']
              end = event['end']
              
              web_total_page_list = []
              for each in range(int(start),int(end)+1):
                  web_total_page_list.append(each)
              
              print(web_total_page_list)
              
              get_handles_new_link(web_total_page_list, s3_key)
      Handler: 'index.lambda_handler'
      Layers: 
        - !Sub 'arn:aws:lambda:${AWS::Region}:336392948345:layer:AWSDataWrangler-Python39:3'
      Timeout: 900
      Role: !GetAtt 'ETLLambdaIAM.Arn'
      Runtime: 'python3.9'
  TransformRawData: # no monitoring needed because function is invoked by StepFunctions
    Type: 'AWS::Lambda::Function'
    Properties:
      MemorySize: 4096
      Code:
        ZipFile: |
          import pandas as pd
          import awswrangler as wr
          import datetime

          def lambda_handler(event, context):
              # load up jobid_list for jobs that was previously scraped
              jobid_list = s3_load_jobid_list()
              print(len(jobid_list))
              # loading up temp data that was scraped
              temp_df = s3_load_temp_df()
              print(temp_df.shape)
              # transform df for new meta data
              temp_meta_df = get_new_meta_data(temp_df)
              # save new meta data
              s3_save_meta_df(temp_meta_df)
              # 
              new_job_df = get_new_job_data(temp_df, jobid_list)
              print(new_job_df.shape)
              # 
              s3_save_jobid(new_job_df)
              # 
              s3_key = s3_save_instance_df(new_job_df)
              return {
                'Payload': {
                  's3_key': s3_key
                }
              }
              
          def s3_load_temp_df() -> pd.DataFrame:
              aws_s3_bucket = 'mcf/temp'
              
              key_list = [
                  'split_2',
                  'split_3',
                  'split_4',
                  'split_5',
                  'split_6',
                  'split_7',
                  'split_8'
                  ]
              
              #output_df = pd.DataFrame
              output_df = wr.s3.read_parquet(f"s3://{aws_s3_bucket}/split_1")
              for key in key_list:
                  
                  temp_df = wr.s3.read_parquet(f"s3://{aws_s3_bucket}/{key}")
                  
                  #temp_df = pd.read_parquet(
                  #        f"s3://{aws_s3_bucket}/{key}",
                  #    engine='pyarrow', 
                  #    compression='snappy',
                  #    )
                  
                  frames = [output_df, temp_df]
                  # Combining the website_new and website_old dataframe into one dataframe 
                  output_df = pd.concat(
                                      frames,
                                      axis=0,
                                      join="outer",
                                      ignore_index=True,
                                      keys=None,
                                      levels=None,
                                      names=None,
                                      verify_integrity=False
                                  )
                  
              return output_df
              
          def s3_save_instance_df(input_df: pd.DataFrame):
              """
              aws s3 saving of scrapped data base on individual run time
              this approach would create single csv file every day or append to existing file
              """
              # setting the filler for the naming convention
              now = datetime.datetime.now() + datetime.timedelta(hours=8)
              year = now.year
              month = '%02d' % now.month
              day = '%02d' % now.day
              
              # setting the S3 bucket name
              aws_s3_bucket = 'mcf'
              
              # setting the S3 object name / file name
              key = f'raw_data/{year}/{month}/{day}'

              try:
                  # reading of csv directly from s3 via awsrangler
                  exist_df = wr.s3.read_parquet(f"s3://{aws_s3_bucket}/{key}")
                  # reading of csv directly from s3 via pandas, achieved via usage of s3fs
                  #exist_df = pd.read_csv(
                  #    f"s3://{aws_s3_bucket}/{key}",
                  #)
                  # Setting the list of dataframe
                  frames = [exist_df, input_df]
                  # Combining the website_new and website_old dataframe into one dataframe 
                  input_df = pd.concat(
                                      frames,
                                      axis=0,
                                      join="outer",
                                      ignore_index=True,
                                      keys=None,
                                      levels=None,
                                      names=None,
                                      verify_integrity=False
                                  )
                  print(f'Per Instance File Found, updating {key} into S3')
              except:
                  print(f'Per Instance File Not Found, saving new file into S3 as {key}')
              
              # saving of parquet directly to s3 via awswrangler
              wr.s3.to_parquet(
                  df=input_df,
                  path=f"s3://{aws_s3_bucket}/{key}",
                  dataset=True,
                  mode="overwrite")
              
              return key

          def s3_load_jobid_list() -> list:
              """
              aws s3 loading of website link csv file in the s3 bucket
              """
              # importing the full scrapped data from s3 bucket
              # setting the S3 bucket name
              aws_s3_bucket = 'mcf'
              
              # setting the S3 object name / file name
              key = 'joblist'

              try:
                  # reading of csv directly from s3 via pandas, achieved via usage of s3fs
                  #output_df = pd.read_parquet(
                  #    f"s3://{aws_s3_bucket}/{key}",
                  #engine='pyarrow', 
                  #compression='snappy',
                  #)
                  #output_list = list(output_df['metadata.jobPostId'])
                  
                  output_df = wr.s3.read_parquet(f"s3://{aws_s3_bucket}/{key}")
                  output_list = list(output_df['metadata.jobPostId'])
                  
              except FileNotFoundError:
                  output_list = []
                  print('JobID File Not Found')

              return output_list

          def get_new_job_data(input: pd.DataFrame, jobid: list) -> pd.DataFrame:
              """
              
              """
              print(len(jobid))
              print(jobid[0])
              print(jobid[-1])
              print(input.shape)
              output_scrapped_df = input[~input['metadata.jobPostId'].isin(jobid)]
              return output_scrapped_df

          def s3_save_jobid(input_df: pd.DataFrame):
              """
              aws s3 saving of website link that was scrapped
              this would continous update the csv file to store all website link that was scrapped
              these website link form as a core reference to identify scrapped or new links
              """
              # filter out the website link from scrapped data
              input_jobid_df = input_df[['metadata.jobPostId']]

              # importing the full scrapped data from s3 bucket
              # setting the S3 bucket name
              aws_s3_bucket = 'mcf'
              # setting the S3 object name / file name
              key = 'joblist'

              try:
                  exist_df = wr.s3.read_parquet(f"s3://{aws_s3_bucket}/{key}")
                  
                  print('JobID File Found, the existing JobID file will be updated')
              except:
                  exist_df = pd.DataFrame
                  print('JobID File Not Found, a new JobID file will be saved')

              frames = [exist_df, input_jobid_df]
              # Combining the website_new and website_old dataframe into one dataframe 
              output_df = pd.concat(
                                  frames,
                                  axis=0,
                                  join="outer",
                                  ignore_index=True,
                                  keys=None,
                                  levels=None,
                                  names=None,
                                  verify_integrity=False
                              )
              
              # saving of parquet directly to s3 via awswrangler
              wr.s3.to_parquet(
                  df=output_df,
                  path=f"s3://{aws_s3_bucket}/{key}",
                  dataset=True,
                  mode="overwrite")

          def get_new_meta_data(input_df: pd.DataFrame) -> pd.DataFrame:
              """
              
              """
              output_meta_df = input_df[[
                  'metadata.jobPostId','metadata.deletedAt','metadata.createdBy',
                  'metadata.createdAt','metadata.updatedAt','metadata.editCount',
                  'metadata.repostCount','metadata.totalNumberOfView','metadata.newPostingDate',
                  'metadata.originalPostingDate','metadata.expiryDate',
                  'metadata.totalNumberJobApplication','metadata.isPostedOnBehalf',
                  'metadata.isHideSalary','metadata.isHideCompanyAddress',
                  'metadata.isHideHiringEmployerName','metadata.isHideEmployerName',
                  'metadata.jobDetailsUrl','metadata.matchedSkillsScore'
              ]]
              return output_meta_df


          def s3_save_meta_df(input_df: pd.DataFrame):
              """
              aws s3 saving of scrapped meta data base on individual run time
              this approach would create single csv file every day
              """
              # setting the filler for the naming convention
              now = datetime.datetime.now() + datetime.timedelta(hours=8)
              year = now.year
              month = '%02d' % now.month
              day = '%02d' % now.day
              
              # setting the S3 bucket name
              aws_s3_bucket = 'mcf'
              
              # setting the S3 object name / file name
              key = f'raw_meta/{year}/{month}/{day}'

              # try to check if S3 have existing file with the same key naming convention, if so load the csv and concat it with input_df before saving back to S3
              # if except FileNotFoundError is raise, it would save as per key naming convention.
              try:
                  # reading of csv directly from s3 via awsrangler
                  exist_df = wr.s3.read_parquet(f"s3://{aws_s3_bucket}/{key}")

                  # Setting the list of dataframe
                  frames = [exist_df, input_df]
                  # Combining the website_new and website_old dataframe into one dataframe 
                  input_df = pd.concat(
                                      frames,
                                      axis=0,
                                      join="outer",
                                      ignore_index=True,
                                      keys=None,
                                      levels=None,
                                      names=None,
                                      verify_integrity=False
                                  )
                  print(f'Meta File Found, Updating file {key} into S3')
              except:
                  print(f'Meta File Not Found, Saving new file into S3 as {key}')
              
              # saving of parquet directly to s3 via awswrangler
              wr.s3.to_parquet(
                  df=input_df,
                  path=f"s3://{aws_s3_bucket}/{key}",
                  dataset=True,
                  mode="overwrite")
      Handler: 'index.lambda_handler'
      Layers:
        - !Sub 'arn:aws:lambda:${AWS::Region}:336392948345:layer:AWSDataWrangler-Python39:3'
      Timeout: 900
      Role: !GetAtt 'ETLLambdaIAM.Arn'
      Runtime: 'python3.9'


  # Eventbridge
  MCFDailyTrigger:
    Type: AWS::Events::Rule
    Properties: 
      Description: 'An automated daily trigger for step function for MCFScraperSM'
      ScheduleExpression: "cron(0 17 * * ? *)"
      State: 'ENABLED'
      Targets: 
      - Arn: !GetAtt 'MCFScraperSM.Arn'
        Id: 'MCFId2022'
        RoleArn: !GetAtt 'MCFScraperEventbridge.Arn'

  # IAM roles
  MCFScraperStateMachine:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: !Sub 'states.amazonaws.com'
          Action: 'sts:AssumeRole'
      ManagedPolicyArns:
      - 'arn:aws:iam::aws:policy/AmazonS3FullAccess'
      - 'arn:aws:iam::aws:policy/CloudWatchFullAccess'
      - 'arn:aws:iam::aws:policy/AWSLambda_FullAccess'
      - 'arn:aws:iam::aws:policy/AWSStepFunctionsFullAccess'
  MCFScraperEventbridge:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: !Sub 'events.amazonaws.com'
          Action: 'sts:AssumeRole'
      ManagedPolicyArns:
      - 'arn:aws:iam::aws:policy/AWSStepFunctionsFullAccess'
  ETLLambdaIAM:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: 'lambda.amazonaws.com'
          Action: 'sts:AssumeRole'
      ManagedPolicyArns:
      - 'arn:aws:iam::aws:policy/AmazonS3FullAccess'
      - 'arn:aws:iam::aws:policy/CloudWatchFullAccess'
      - 'arn:aws:iam::aws:policy/AWSLambda_FullAccess'
      - 'arn:aws:iam::aws:policy/AWSStepFunctionsFullAccess'
